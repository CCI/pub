\documentclass[conference]{IEEEtran}

\usepackage{cite}

\usepackage{url}

\usepackage{color}
\usepackage{xcolor}
\usepackage{amsmath}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor Myri-com Myrinet-Express Path-Scale}

\begin{document}
%
\title{CCI: Common Communication Interface}
% 
\author{\IEEEauthorblockN{
    Scott Atchley\IEEEauthorrefmark{1},
    David Dillow\IEEEauthorrefmark{1},
    Galen Shipman\IEEEauthorrefmark{1},\\
    Patrick Geoffray\IEEEauthorrefmark{2},
    Jeffrey M.\ Squyres\IEEEauthorrefmark{3} and
    George Bosilca\IEEEauthorrefmark{4}}
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Oak Ridge National Laboratory, Oak Ridge, TN}
  \IEEEauthorblockA{\IEEEauthorrefmark{2}Myricom, Inc., Arcadia, CA}
  \IEEEauthorblockA{\IEEEauthorrefmark{3}Cisco Systems, Inc., San Jose, CA}
  \IEEEauthorblockA{\IEEEauthorrefmark{4}University of Tennessee, Knoxville, TN}}

% make the title area
\maketitle

\begin{abstract}
  Striving for performance or portability, complexity or simplicity,
  multiple API for exchanging data between peers are available
  today. While each of them has interesting and unique features,
  developers using them to build more complex software
  infrastructures, found points where the API failed to
  deliver critical features required in today's complex software
  development environment. 

  In this paper we introduce a novel API, based on features already
  available on other communication API, which expose a minimalistic
  interface while allowing the upper levels to take advantage of a
  significant number of features. We based the proposed API on
  requirements from our diverse backgrounds, triving to keep the high
  level complexity at a minimum. In this paper we prove that
  portability, and simplicity at the API level, while providing
  scalability and robustness are features that can be provided by a
  communication library with a minimal impact on the performance of
  the underlying communication protocol. As such, we expect such an
  interface to facilitate the development of new network hardware,
  minimize the overhead imposed on the host operating system while
  allowing application and library developpers to reach two of the
  most challenging requirement for the dynamic distributed tomorrow's
  applications: minimize the latency and maximize the bandwidth,

\end{abstract}

% no keywords

% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

% insert draft notes - highlights with a yellow box and prefixes with "Note: "
\newcommand{\note}[1]{\colorbox{yellow!50}{Note: #1}}

% function names - adds (), sets tt
\newcommand{\f}[1]{\texttt{#1{\kern-2pt}()}}

\section{Introduction}
Introduction text goes here.

\section{State of the Art}
Over the years, many communication interfaces have come and gone. The few that have
remained and seen wide-spread adoption are BSD sockets\cite{bsd}, the Message Passing
Interface (MPI)\cite{mpi}, and some vendor-specific application programming interfaces
(API).

\subsection{Sockets} The socket interface is the most widely used by far. All major
operating systems provide support for sockets and the Internet and all the services it
provides would not exist without it. The popularity of sockets can be attributed to:

\begin{itemize}
\item Simple API
\item Robustness
\item Asynchronous operations
\end{itemize}

The API provides stream and datagram based modes, connection-oriented and connection-less
modes, and client/server semantics for connection-oriented modes. Based on the transport,
the API can supports multiple delivery modes (unicast, multicast, and/or broadcast). The
API does not provide for collective communication nor does it provide one-sided
operations.

Sockets implementations are mature and well understood. It does not assume or require
special hardware features (nor can it exploit them if they exist).

Both sends and receives are buffered allowing operations to complete quickly (if buffer
space is available for sends or data exists in the buffer for receives). Applications may
also choose to not block if send buffers are full or receive buffers are empty if need be.
The downside of buffering is more work is required by the CPU which can result in lower
throughput over the network.

Sockets, when used with SOCK\_STREAM (i.e. TCP), do not scale well since each connection
requires a separate socket and each socket has its own send and receive buffers.

\subsection{MPI} In the high performance computing (HPC) arena, MPI is the dominant
interface for inter-process communication. Designed for maximum scalability, MPI has a
richer, but more complicated API.

It provides point-to-point (i.e. send-recv or two-sided semantics), collective, and
one-sided operations. For point-to-point communication, MPI provides a variety of modes
including blocking and non-blocking, synchronous and asynchronous, as well as \emph{ready}
mode (only send if a matching receive has been posted).

Multiple implementations exist\cite{ompi, mpich2, mvapich, intel-mpi, platform-mpi}
which support multiple operating systems and/or interconnects.  Rather than connections,
the API uses the notion of communication groups (i.e.  communicators) which include all
processes (MPI\_COMM\_WORLD) and may be split to include subsets of processes. MPI does
provide a notion of dynamic process management which includes MPI\_Comm\_accept and
MPI\_Comm\_connect, but this is less mature and much less used.

The MPI standard does not define an underlying network protocol and each MPI
implementation has written its own network abstraction layer (NAL). These NALs typically
support sockets as well as one or more vendor specific APIs.

MPI is a less mature technology compared to sockets likely due to its
relative complexity (over 100 functions), niche market penetration
(HPC), and relative youth. While MPI semantics are fairly well defined
by the MPI standard, implementations vary in their compliance to MPI
semantics and may exhibit differing behavior when the standard is
silent on a specific semantic. Applications that rely upon the
semantic embodied in a particular implementation may not be portable
across other MPI implementations. MPI provides a rigid fault model in
which a process fault within a communication group imposes failure to
all processes within that communication group
(MPI\_ERRORS\_ABORT). Although work has been done on fault-tolerant
MPI\cite{ft-mpi, mpi-ft}, that work has yet to be adopted by the
broader HPC community. 

The MPI standard does remains silent on a number of areas that are
often performance critical. For basic send/recv operations MPI
implementations often adopt two common strategies that attempt to
balance buffering and communication overhead. ``Eager'' mode will send
data immediately to the receiver regardless of the receiver having
posted a matching receive. The data is buffered on the receiver if the
matching receive is not posted upon receipt of the
data. ``Rendezvous'' mode defers sending the data until after a
matching receive is through the use of a ready-to-send/clear-to-send
(RTS/CTS) protocol. Performance of an MPI application can therefore be
largely implementation dependent and may vary even within a single
implementation depending on the MPI configuration settings used for a
particular invocation. Perhaps more importantly, this ambiguity can
result in receiver buffer overruns or out-of-memory (OOM) faults on
the receiver. 

It has limited support for one-sided operations due to the lack of
memory registration.\note{Patrick, can you elaborate?}

\subsection{Vendor APIs} There are numberous vendor- and organization-specific APIs
including Infiniband's Verbs\cite{ofa-verbs}, Cray/Sandia's Portals\cite{portals}, Qlogic's
PSM\cite{psm}, Myricom's MX\cite{mx}, LBL's GASNet\cite{gasnet}, and so on.  Overall, they
provide a lot of choice, but none is perfect (i.e. none have convinced they others to
adopt their API).  Since most are targeted to specific hardware, the APIs tend to be more
complicated.

\note{Mention DAPL}.

Based on the earlier VIA specification\cite{via}, the Infiniband standard does not specify
an API; it only specifies which \emph{verbs} must be supported. After many vendors created
separate Verbs APIs, they eventually coalesced into the Open Fabric Association's (OFA)
Verbs. OFA's Verbs has had the broadest adoption, yet it still represents less than 43\%
of the machines on the Top 500\cite{top500} with some, but much less, adoption outside of
HPC.

Verbs has support for (small) two-sided and one-sided operations. All operations are
asynchronous. Verbs has support for reliable and unreliable modes, connection-oriented
and connection-less. Verbs does not support buffering; all receives must be posted before
sending. Also, all data movement operations require pinning the memory in advance. For the
most common reliable, connection-oriented communication, Verbs requires that the two
processes establish a queue-pair (QP), which is much more cumbersome than socket's
\f{connect} and \f{accept}. Lastly, requiring QPs between all processes has memory scaling
issues. Work has been done with shared receive queues (SRQ) in MPI\cite{srq} to help
increase the scalability of Verb's based systems.\note{can someone review all of this?}

The Portals API provides one-sided semantics (i.e.  Put/Get), uses match tags to steer
messages to the correct buffers. The API is connection-less and leaves it up to the NAL to
maintain any necessary connection state internally. Portals is mostly used on the large
Cray systems such as ORNL's Jaguar\cite{jaguar} and LANL's Cielo\cite{cielo}.  The
Lustre distributed file system NAL, LNET, was originally based on Portals\cite{lnet}.

Both designed for implementing MPI, Myricom's MyrinetExpress (MX) and Qlogic's PathScale
Messages (PSM) have many similarities. Both provide a two-sided interface which uses
buffering for smaller messages and remote direct memory access for larger messages.  Both
are connection-less in that the target does not have to accept.  Both provide reliable,
in-order matching, but out-of-order completion.

\section{Design Goals}
In setting out to design a new communication's interface, we had several goals 
in mind: portability, simplicity, performance, scalability, and robustness.

\subsection{Portability}
Application and middleware developers do not have the resources to continuously 
port their code on different communication interfaces. 
Selecting a vendor-specific API reduces competition in the market place, thus 
increasing prices and adding the risk of business failure or market disruptions. It 
also slows down the adoption of new and improved technology. 
Similarly, vendors do not have the resources to properly support a large set 
of middlewares. The whole ecosystem would clearly benefit from a truly unified 
communication layer. 
Sockets and MPI both provide such a high-level of portability. 
For any new communication interface to gain acceptance in the broader 
community, it needs to provide a similar breadth of implementations on 
currently available hardware, by supporting the semantics that are common 
to most vendor APIs.

\subsection{Simplicity}
Simplicity is paramount to the success of a programming interface. Critical 
mass cannot be reached by limiting the targeted audience to a few networking 
experts. However, ease of use involves many elements beyond just expertise. 
Code size is a common, albeit subjective, metric used to compare programming 
interfaces. The rationale is that larger codes are harder to debug and 
maintain. For example, an analysis of the OpenMPI implementation shows great 
differences between the seven supported communication APIs (excluding self and 
shared memory). The total lines of code of each Byte Transfer Layer (BTL) is 
listed in Table \ref{tab:btl}. The Verbs BTL is the largest, triple the size 
of the Sockets BTL, second largest, and 5 to 6 times larger than the BTLs of 
vendor interfaces. 

\begin{table}[htbp] \centering
\caption{Lines of Code per BTL}
\label{tab:btl}
\begin{IEEEeqnarraybox}[\IEEEeqnarraystrutmode\IEEEeqnarraystrutsizeadd{2pt}{1pt}]{v/s/v/s/v}
\IEEEeqnarrayrulerow\\ &\mbox{BTL}&&Lines of code&\\
\IEEEeqnarraydblrulerow\\
\IEEEeqnarrayseprow[3pt]\\ &Elan&&1,656&\IEEEeqnarraystrutsize{0pt}{0pt}\\
\IEEEeqnarrayseprow[3pt]\\
\IEEEeqnarrayrulerow\\
\IEEEeqnarrayseprow[3pt]\\ &MX&&2,333&\IEEEeqnarraystrutsize{0pt}{0pt}\\
\IEEEeqnarrayseprow[3pt]\\
\IEEEeqnarrayrulerow\\
\IEEEeqnarrayseprow[3pt]\\ &Portals&&2,469&\IEEEeqnarraystrutsize{0pt}{0pt}\\
\IEEEeqnarrayseprow[3pt]\\
\IEEEeqnarrayrulerow\\
\IEEEeqnarrayseprow[3pt]\\ &GM&&2,779&\IEEEeqnarraystrutsize{0pt}{0pt}\\
\IEEEeqnarrayseprow[3pt]\\
\IEEEeqnarrayrulerow\\
\IEEEeqnarrayseprow[3pt]\\ &Sockets (TCP)&&4,192&\IEEEeqnarraystrutsize{0pt}{0pt}\\
\IEEEeqnarrayseprow[3pt]\\
\IEEEeqnarrayrulerow\\
\IEEEeqnarrayseprow[3pt]\\ &UDAPL&&6,208&\IEEEeqnarraystrutsize{0pt}{0pt}\\
\IEEEeqnarrayseprow[3pt]\\
\IEEEeqnarrayrulerow\\
\IEEEeqnarrayseprow[3pt]\\ &OpenIB (Verbs)&&13,994&\IEEEeqnarraystrutsize{0pt}{0pt}\\
\IEEEeqnarrayseprow[3pt]\\
\IEEEeqnarrayrulerow
\end{IEEEeqnarraybox}
\end{table}

Another indicator of complexity is the number of functions available. Choice 
is good but too much choice is worse. Fortunately, software programmers are 
efficient at reducing overly complex interfaces to a minimum set of useful 
semantics.
For example, MPI specifies over 200 functions but the vast majority of MPI 
applications only use a fraction of them. Similarly, relative simplicity was 
the main drive behind the wide adoption of the Socket interface. 
A new common communication interface should aspire to find the right balance 
between richness of semantics and ease of use.

\subsection{Performance}
Performance is major drive for innovation in networking, as much in HPC as 
in Cloud Computing. All modern network technologies leverage common techniques 
developed in the last two decades: OS-bypass, zero-copy, one-sided and asynchronous operations.

\emph{OS-bypass} allows direct interaction between the application and 
a virtualized instance of the network hardware, without involving the 
operating system. 
This technique is essential for low latency, as it removes the need for 
interrupts in the critical path. Furthermore, a process or a thread blocking 
in the kernel is often scheduled on a different core when awakened. Avoiding the 
operating system can greatly improves NUMA locality. 
To support OS-bypass, the network adapter must be able to demultiplex incoming 
packets into corresponding queues in each application. Most of this 
functionality is commonly used in Ethernet adapters that support Receive Side 
Scaling (RSS).

\emph{Zero-copy} reduces CPU overhead and increases bandwidth by eliminating 
memory copies in the critical path. The network adapter fetches or delivers 
data directly into the memory space of the application via Direct Memory Access 
(DMA) operations. To this end, the related memory pages must be pinned so 
that the network adapter can safely access it. 
An important drawback of zero-copy is its synchronous nature. Since there is 
no intermediate copies, the memory on the send side cannot be reused until 
the data has safely been delivered on the receive side (or at least put on 
the wire for unreliable connections).

Zero-copy is often confused with \emph{one-sided operations}, which allow a 
communication to completed without the involvement of the application thread 
on the remote side. All of the required information, mainly the remote address 
of the data to access, is provided on the origin side. One-sided operations 
may or may not be associated with zero-copy, and may use the help of a 
progression thread on the target side. Similarly, zero-copy may be implemented 
with receive matching instead or remote addressing, as it is the case with MX 
and Portals.

\emph{Asynchronous operations} are used to decouple the initiation of a 
communication from its completion. They allow the cost of communication to be 
overlapped with computation. Interestingly, enough computation could completely 
overlap all communications, possibly making network bandwidth irrelevant. 
More practically, asynchronous operations allow to initiate concurrent 
data movements without blocking the application thread.

To deliver the best performance, a new communication interface should present
semantics that can efficiently leverage all these techniques as provided by 
modern high-speed networks.

\subsection{Scalability}
Projections for exascale systems in HPC include hundreds of thousands of nodes 
and millions of cores\cite{dongarra:exascale-talk-2010}. In the commercial 
space, Cloud Computing datacenters are fast approaching these levels. 
In this context, scalability is an important requirement. The time and space 
overhead of a scalable communication interface should not grow linearly with 
the number of communicating partners. Sockets are inefficient in both 
dimensions, as buffers and file handles are allocated for each new Socket. 
Through adaptive socket buffers and use of epoll(), Socket implementations 
have so far managed to reasonably handle large number of connections. 
MPI is inherently more scalable and it has successfully been deployed on large 
HPC machines. However, it is not clear if MPI in its present form can 
efficiently scale to millions of cores. 
Scalability of the Verbs interface was originally quite poor due to it's Queue 
Pair model. MPI implementations used various techniques such as connection on 
demand\cite{connect-on-demand} and dynamic buffer management\cite{dynamic-buffer} 
to work around the QPs memory footprint problem. Scalability later improved 
with the addition of Shared Receive Queues (SRQ), but distinct QPs are still 
required on the send side. To address the Cloud Computing and Exascale 
requirements, a new communication interface should aim for constant buffer 
and polling overhead, independently of the number of nodes in the fabric.

\subsection{Robustness}
Hardware and software failures occur everywhere, all the time, especially on 
large systems. Ignoring them is unacceptable. Unfortunately, this is exactly 
how most MPI implementations handle such errors. There have been several 
efforts aimed at designing fault-tolerant MPI libraries and add fault recovery 
to the MPI specification, without success so far. The loose semantic about 
status completions was actually a benefit in making MPI a simpler interface, 
developers would send messages and trust MPI to always deliver them. 
Unfortunately, real-world applications eventually had to implement 
checkpoint/restart functionality to work around this problem and it is the 
only practical solution available today on large HPC systems. 
Both Sockets and Verbs fare better than MPI on this issue. They use connections 
to represent the state of communication channels. Connections are essential 
for robustness, they contain faults and allow for their recovery by reseting 
the state of the affected communication channels. 
Unfortunately, both Sockets and Verbs associate buffers to a connection, which 
negatively affects scalability. A new communication interface should provide 
connection-oriented semantics without per-connection resources. 

Communication reliability is often seen as a way to improve overall robustness. 
For some applications such as Media Content Delivery (IPTV), Financial Trading 
(HFT) or system-health monitoring, the provided reliability may be incompatible 
with their timing requirements. 
Furthermore, most scalable multicast implementations are unreliable. For these 
reasons, a large share of applications use unreliable connections. 
A new communication interface should provide different type of connection, 
including multicast. 


\section{The CCI Interface}
In this section, we provide a brief overview of the CCI API to allow us to discuss how CCI
can meet the goals outlined above. Each function call is prefixed by \texttt{cci\_} which
we omit below for brevity.

\subsection{Endpoints}
All communication in CCI revolves around an endpoint. Each endpoint has some number of
device-sized buffers available for sending and receiving messages of small, unexpected
messages. The application calls \f{create\_endpoint} and \f{destroy\_endpoint},
respectively, to obtain or release an endpoint. The application may alter the number of
send and/or receive buffers using \f{get\_opt} and \f{set\_opt}.

\subsection{Connections}
CCI defines a \emph{connection} struct which includes the maximum send size negotiated by
the two instances of CCI, a pointer to the owning endpoint, and the connection attribute.

As mentioned above, some applications may need reliable delivery while other may not.
Among applications needing reliable delivery, some may need in-order completion (e.g.
traditional SOCK\_STREAM semantics) and others may accept out-of-order completion as long
as communications are initiated in-order (e.g. MPI point-to-point).

In order to provide applications with the level of service appropriate for their needs,
CCI provides multiple types of connection attributes:

\begin{itemize}
\item Reliable with Ordered completion (RO)
\item Reliable with Unordered completion (RU)
\item Unreliable with Unordered completion (UU)
\item Unreliable with Unordered completion with multicast send (UU\_MC\_TX)
\item Unreliable with Unordered completion with multicast receive (UU\_MC\_RX)
\end{itemize}

If a process needs a mix of types, it is allowed to open multiple connections to the other
process.

It is also an opportunity to allow the client to pass credentials to the
server if authentication and/or authorization is required and it is an opportunity for the
two sides to negotiate message sizes, the number of messages in flight, and other internal
information as appropriate.

\subsection{Connection Establishment}
As we previously discussed, part of the robustness of the sockets API is its client/server
connection semantics. CCI mirrors those semantics. A process willing to accept connections
will first \f{bind} a device to a name service at a specified \emph{port} and a
\emph{backlog} parameter. The call returns a pointer to a \emph{service}. When a server no
longer wishes to receive connection requests, it can \f{unbind} from the service.

To initiate a connection, the client calls \f{connect} with parameters including an
endpoint, a string URI for the server, the port, optionally a pointer to a limited sized
payload and its length, the connection attribute, a pointer to an optional application
context, and a timeout.

The server then polls for connection requests using \f{get\_conn\_request} passing in the
service pointer. If one is ready, it returns a \emph{conn\_req} struct which contains an
array of compatible devices, the number of devices in the array, a pointer to the
application payload and its length if the client sent it, and the requested connection
attribute.

The server then calls either \f{accept} or \f{reject}. The \f{accept} call binds the
connection request to an endpoint previously created from one of the compatible devices
and returns a connection pointer. The client gets an OTHER event with the type
CONNECT\_SUCCESS. If the server calls \f{reject}, the client get an other event with the
type CONNECT\_REJECTED.  On the server, the connection request is stale after either call.
If the server does not reply within the timeout set in the client's \f{connect}, the
client gets an OTHER event with a type of CONNECT\_TIMEOUT. When a process no longer needs
a connection, it can call \f{disconnect}.

\subsection{Active Messages}

\note{Below is mostly Patrick's text from his outline}
Buffered messages as in sockets provide asynchronism but consume time and 
space. Asynchronism is fundamental for scalability, but space is bounded and 
time is precious. 
Matching interfaces (as used in MPI) are powerful but very complex. The 
matching semantics (wildcards) may require coherent matching implementation, 
preventing effective offload (has to be done in the host). Furthermore, 
matching generally requires support for unexpected messages, which consumes 
time and space. Finally matching interfaces are stateful, which is bad for 
fault-tolerance and offload.  Socket's SOCK\_STREAM semantic is akin to 
ordered matching, all messages demultiplexed to the same destination socket 
are treated as unexpected messages until consumed.

Active Messages is well known solution but has two problems: async handlers 
and reassembly of large messages.  Async handlers are unbounded in term of 
running time. CCI uses events instead.  Messages larger than MTU requires 
reassembly: means stateful (bad), means unexpected buffers (bad). The solution 
is to limit message size to MTU, segmentation/reassembly in application.

Once the connection is established, the two processes can start communicating. CCI
provides two methods, active messages and remote memory access (RMA), which we discuss in
the next section.

CCI's version of active messages does not fully mirror Active Messages\cite{am} (AM). Like
the original AM, CCI's active messages have a maximum size that is device dependent.
Ideally, the size is equal to the link MTU (less wire headers). The driving idea to
limiting the message size to a single MTU is that future networks may have many paths
through the network due to fabrics with high-radix switches and/or NICs with multiple
ports connected to redundant switches for fault-tolerance.  Limiting the active message
size limited to a single MTU vastly simplifies the requirements for message completion ---
either it arrives or it does not.

Where CCI differs from the original Active Messages is handling of incoming messages.  In
Active Messages, the message contains an address of the handler that will process it,
which assumes all processes have identical memory spaces.  The difficulty with invoking
handlers is there is no bound on how long the handler will run.  While running, the
communication library cannot process any more messages and could lead to dropping
messages. Instead, CCI returns an event of type RECV. The application can get the event
and hold it without blocking CCI from continuing to service other communications.

The \f{send} parameters include the connection, header and data pointers and their
respective lengths, an application context pointer, and flags. Either or both of the
pointers may be NULL. The header is currently limited to a maximum length of 32 bytes. The
context pointer is returned in the SEND completion event and can be used to allow the
application to retrieve its internal state.


\subsection{Remote Memory Access}
Clearly, messages limited to a single MTU will not meet the needs of all applications.
Applications such as file systems which need to move large, bulk messages need much more.
To accommodate them, CCI also provides remote memory access (RMA). RMA transfers are only
allowed on reliable connections.

Before using RMA, the process needs to explicitly register the memory. CCI provides
\f{rma\_register} which takes pointers to the endpoint, the connection, and the start of
the region to be registered as well as the length of the region and it returns a RMA
handle. If the connection pointer is set, RMA operations on that handle will be limited to
that one connection. If the connection is NULL, then RMA operations on that handle will be
limited to any connection on that endpoint.  When a process no longer needs to RMA in to
or out of the region, it passes the handle to \f{rma\_deregister}.

For a RMA transfer to take place, both processes must register their local memory and they
need to pass the handle of the target process to the initiator process using one or more
active messages. The full \f{rma} signature is:

\begin{verbatim}
int rma(cci_connection_t *connection,
        void *header_ptr,
        uint32_t header_len,
        uint64_t local_handle,
        uint64_t local_offset,
        uint64_t remote_handle,
        uint64_t remote_offset,
        uint64_t data_len,
        void *context,
        int flags);
\end{verbatim}

The call takes the connection pointer, an optional header pointer and length, the local
RMA handle and offset, the remote RMA handle and offset, the transfer length, an
application context pointer, and a set of flags.

If the header pointer and length are set, the initiator will send a completion message to
the target that arrives as an active message with the header set and no data payload. Like
with \f{send}, the header length is limited to 32 bytes.

The flag options include:

\begin{description}
\item BLOCKING (see \f{send})
\item SILENT (see \f{send})
\item READ allows data to move from remote to local memory.
\item WRITE allows data to move from local to remote memory.
\item FENCE means wait for all previous RMA operations to complete before performing this
operation and all following RMA operations.
\end{description}

CCI does not guarantee delivery order within an operation (i.e. no last-byte-written-last
mandate), but order is guaranteed between data delivery and the remote receive event if
the header is specified.

\note{thoughts?} Explicit memory registration which is missing from MPI.\note{why is this
good, George?} Simpler semantics and coding compared to Verbs.\note{is this true, Dave?}

\section{Status and Evaluation}
We have two partial, proof-of-concept implementations, one over UDP sockets and
one over Portals 3.3.

The socket prototype driver opens one SOCK\_DGRAM (UDP) socket per CCI endpoint
and has to provide reliability (acknowledgement and retransmission on loss).
The socket driver also has to multiplex connections over the single socket.
Enough functions are complete including RMA write to allow simple pingpong
tests to exercise the CCI API.

The Portals implementation also implements enough functions to run a pingpong
test with active messages. Since Portals assumes a reliable interconnect, the
only difference between a CCI UU connection and a RO/RU connection is that we
complete a UU send when we receive the Portals' SEND\_END event which indicates
that the sender's buffer is no longer needed (i.e. the data is on the wire).
For a reliable connection, we report a completion when we get the Portals' ACK
of receipt at the peer. We wrote a native Portals pingpong and compared it to
the CCI pingpong performance. We ran tests on a Cray XT6 and locked the
processes to the same cores for both tests. The CCI over Portals
\begin{math}\frac{1}{2}\end{math}RTT was less than 200 ns more than the antive
Portals' performance, 6.0 $\mu$\textit{s} versus 5.8 us.



Given the authors work with MX, GM, sockets, and Portals, we expect that a BTL
implementation using CCI would be smaller than MX and hopefully smaller than
Elan.

\subsection{Portability}

\subsection{Performance}

\subsection{Scalability}

For the Portals driver, each connection requires 104 bytes on 64-bit machines
(20 bytes for the public CCI connection struct, 20 bytes for the private CCI
struct, and 64 bytes for the Portals driver connection struct).  The sock driver
needs 140 bytes for each connection.

\subsection{Robustness}

%** elan **
    %1656 total
%** gm **
    %2779 total
%** mx **
    %2333 total
%** ofud **
    %2332 total
%** openib **
   %13994 total
%** portals **
    %2469 total
%** self **
     %985 total
%** sm **
    %2189 total
%** tcp **
    %4192 total
%** udapl **
    %6208 total


\section{Conclusion}
The conclusion goes here.


% use section* for acknowledgement
\section*{Acknowledgment}

The authors would like to thank...


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{30}

\bibitem{mpi-ft}
R. Batchu et al, ``MPI/FT TM : Architecture and taxonomies for fault-tolerant,
message-passing middleware for performance-portable parallel computing'', in proceedings
of \emph{1st IEEE International Symposium of Cluster Computing and the Grid}, 2001, pp.
26-33.

\bibitem{lnet}
P. Braam, P. Schwan, and R. Brightwell, ``Portals and Networking for the Lustre File
System'', in proceedings of \emph{IEEE International Conference on Cluster Computing},
2002.

\bibitem{portals}
R. Brightwell, R. Reisen, B. Lawry, and A. B. Maccabe, ``Portals 3.0: Protocol Building
Blocks for Low Overhead Communication'', in proceedings of \emph{2002 Workshop on
Communication Architecture for Clusters}, 2002.

\bibitem{cielo}
Cielo, Los Alamos National Lab, \url{http://www.lanl.gov/programs/asc/cielo_capability.shtml}.

\bibitem{dongarra:exascale-talk-2010}
J. Dongarra, ``Impact of Architecture and Technology for Extreme Scale on Software
and Algorithm Design'', in the proceedings of \emph{The Department of Energy Workshop on
Cross-cutting Technologies for Computing at the Exascale}, Feb. 2010.

\bibitem{ft-mpi}
G. Fagg and J. Dongarra, ``FT-MPI: Fault Tolerant MPI, supporting dynamic applications in
a dynamic world'', 2000.

\bibitem{gasnet}
GASNet, \url{http://gasnet.cs.berkeley.edu/}.

\bibitem{intel-mpi}
Intel MPI Library, Intel, Inc.,
\url{http://software.intel.com/en-us/articles/intel-mpi-library/}.

\bibitem{jaguar}
Jaguar, National Center for Computational Science, Oak Ridge National Lab,
\url{http://www.nccs.gov/computing-resources/jaguar/}.

\bibitem{mpi}
Message Passing Interface Forum. ``MPI: A message-passing interface standard'',
\emph{Technical Report}, 1994.

\bibitem{mpich2}
MPICH2 development team, MPICH2, \url{http://www.mcs.anl.gov/mpi/mpich2}.

\bibitem{mvapich}
MVAPICH, Department of Computer Science and Engineering, Ohio State University,
\url{http://mvapich.cse.ohio-state.edu/}.

\bibitem{mx}
MyrinetExpress (MX), Myricom, Inc., \url{http://www.myri.com/}.

\bibitem{ofa-verbs}
Open Fabrics Alliance, \url{http://www.openfabrics.org}.

\bibitem{ompi}
Open MPI: Open source high performance computing, \url{http://www.open-mpi.org}.

\bibitem{platform-mpi}
Platform MPI, Platform Computing, Inc.,
\url{http://www.platform.com/cluster-computing/platform-mpi}.

\bibitem{psm}
PathScale Messages (PSM), Qlogic, Inc.,
\url{http://www.qlogic.com/SiteCollectionDocuments/Products/Products_RightNAV_pdfs/infiniband%20hcas/IB6054601-00.pdf}.

\bibitem{srq}
S. Sur et al, ``Shared Receive Queue Based Scalable MPI Design for InfiniBand Clusters'',
in the proceedings of \emph{International Parallel and Distributed Processing Symposium
(IPDPS)}, 2006.

\bibitem{top500}
Top 500 Interconnect Family share for 11/2010,
\url{http://www.top500.org/stats/list/36/connfam}.

\bibitem{via}
T. von Eicken and W. Vogels, ``Evolution of the Virtual Interface Architecture'', in
\emph{IEEE Computer}, Vol. 31, Nov. 1998, pp. 61-68. 




\end{thebibliography}

% that's all folks
\end{document}
