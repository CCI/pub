\documentclass[conference]{IEEEtran}

\usepackage{cite}

\usepackage{url}

\usepackage{color}
\usepackage{xcolor}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor Myri-com Myrinet-Express Path-Scale}

\begin{document}
%
\title{CCI: Common Communication Interface}
% 
\author{\IEEEauthorblockN{Scott Atchley\IEEEauthorrefmark{1}, 
David Dillow\IEEEauthorrefmark{1}, 
Galen Shipman\IEEEauthorrefmark{1}, 
Patrick Geoffray\IEEEauthorrefmark{2} and 
Jeffrey M.\ Squyres\IEEEauthorrefmark{3}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Oak Ridge National Laboratory, Oak Ridge, TN}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Myricom, Inc., Arcadia, CA}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Cisco Systems, Inc., San Jose, CA}}

% make the title area
\maketitle

\begin{abstract}
The abstract goes here.

\end{abstract}

% no keywords

% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

% insert draft notes - highlights with a yellow box and prefixes with "Note: "
\newcommand{\note}[1]{\colorbox{yellow!50}{Note: #1}}

\newcommand{\f}[1]{\texttt{#1{\kern-2pt}()}}

\section{Introduction}
Introduction text goes here.

\section{Current State of the Art}
Over the years, many communication interfaces have come and gone. The few that have
remained and seen wide-spread adoption are BSD sockets\cite{bsd}, the Message Passing
Interface (MPI)\cite{mpi}, and some vendor-specific application programming interfaces
(API).

\subsection{Sockets} The socket interface is the most widely used by far. All major
operating systems provide support for sockets and the Internet and all the services it
provides would not exist without it. The popularity of sockets can be attributed to:

\begin{itemize}
\item Simple API
\item Robustness
\item Asynchronous operations
\end{itemize}

The API provides stream and datagram based modes, connection-oriented and connection-less
modes, and client/server semantics for connection-oriented modes. Based on the transport,
the API can supports multiple delivery modes (unicast, multicast, and/or broadcast). The
API does not provide for collective communication nor does it provide one-sided
operations.

Sockets implementations are mature and well understood. It does not assume or require
special hardware features (nor can it exploit them if they exist).

Both sends and receives are buffered allowing operations to complete quickly (if buffer
space is available for sends or data exists in the buffer for receives). Applications may
also choose to not block if need be. The downside of buffering is more work is required by
the CPU which can result in lower throughput over the network.

\subsection{MPI} In the high performance computing (HPC) arena, MPI is the dominant
interface for inter-process communication. Designed for maximum scalability, MPI has a
richer, but more complicated API.

It provides point-to-point (i.e. send-recv or two-sided semantics), collective, and
one-sided operations. For point-to-point communication, MPI provides a variety of modes
including blocking and non-blocking, synchronous and asynchronous, as well as \emph{ready}
mode (only send if a matching receive has been posted).

Multiple implementations exist\cite{ompi, mpich2, mvapich, intel-mpi, platform-mpi}
which support multiple operating systems and/or interconnects.  Rather than connections,
the API uses the notion of communication groups (i.e.  communicators) which include all
processes (MPI\_COMM\_WORLD) and may be split to include subsets of processes. MPI does
provide a notion of dynamic process management which includes MPI\_Comm\_accept and
MPI\_Comm\_connect, but this is less mature and much less used.

The MPI standard does not define an underlying network protocol and each MPI
implementation has written its own network abstraction layer (NAL). These NALs typically
support sockets as well as one or more vendor specific APIs.

MPI is a less mature interface compared to sockets. MPI is more fragile to use than
sockets. If an application using MPI encounters a communications fault, the application
typically aborts. Although work has been done on fault-tolerant MPI\cite{ft-mpi, mpi-ft},
that work has yet to be adopted by the broader HPC community.

The MPI standard does not specify buffering semantics (i.e. when to use eager versus
rendezvous mode). It has limited support for one-sided operations due to the lack of
memory registration.\note{Patrick, can you elaborate?}

\subsection{Vendor APIs} There are numberous vendor- and organization-specific APIs
including Infiniband's Verbs\cite{ofa-verbs}, Cray/Sandia's Portals\cite{portals}, Qlogic's
PSM\cite{psm}, Myricom's MX\cite{mx}, LBL's GASNet\cite{gasnet}, and so on.  Overall, they
provide a lot of choice, but none is perfect (i.e. none have convinced they others to
adopt their API).  Since most are targeted to specific hardware, the APIs tend to be more
complicated.

Based on the earlier VIA specification\cite{via}, the Infiniband standard does not specify
an API; it only specifies which \emph{verbs} must be supported. After many vendors created
separate Verbs APIs, they eventually coalesced into the Open Fabric Association's (OFA)
Verbs. OFA's Verbs has had the broadest adoption, yet it still represents less than 43\%
of the machines on the Top 500\cite{top500} with some, but much less, adoption outside of
HPC.

Verbs has support for (small) two-sided and one-sided operations. All operations are
asynchronous. Verbs has support for reliable and unreliable modes, connection-oriented
and connection-less. Verbs does not support buffering; all receives must be posted before
sending. Also, all data movement operations require pinning the memory in advance. For the
most common reliable, connection-oriented communication, Verbs requires that the two
processes establish a queue-pair (QP), which is much more cumbersome than socket's
\f{connect} and \f{accept}. Lastly, requiring QPs between all processes has memory scaling
issues. Work has been done with shared receive queues (SRQ) in MPI\cite{srq} to help
increase the scalability of Verb's based systems.\note{can someone review all of this?}

The Portals API provides one-sided semantics (i.e.  Put/Get), uses match tags to steer
messages to the correct buffers. The API is connection-less and leaves it up to the NAL to
maintain any necessary connection state internally. Portals is mostly used on the large
Cray systems such as ORNL's Jaguar\cite{jaguar} and LANL's Cielo\cite{cielo}.  The
Lustre distributed file system NAL, LNET, was originally based on Portals\cite{lnet}.

Both designed for implementing MPI, Myricom's MyrinetExpress (MX) and Qlogic's PathScale
Messages (PSM) have many similarities. Both provide a two-sided interface which uses
buffering for smaller messages and remote direct memory access for larger messages.  Both
are connection-less in that the target does not have to accept.  Both provide reliable,
in-order matching, but out-of-order completion.

\section{Goals for CCI}
In setting out to design a new communication's interface, we had several goals in mind:
portability, simplicity, performance, scalability, and robustness.

\subsection{Portability}
For any new interface to gain acceptance in the broader (HPC and non-HPC) community, it
needs to provide the semantics that are common to most vendor interfaces.\note{we need
more here}

\subsection{Simplicity}
The socket interface is the reference. It is flexible without being overly
complicated.\note{we need more here}

\subsection{Performance}
Ideally, an API would provide asynchronous operations to allow overlap between computation
and communication. Buffering may or may not be needed to provide asynchrony. The API would
provide support for zero-copy, OS-bypass when hardware support is available. Lastly, it
would provide support for one-sided operations.

\subsection{Scalability}
Projections for exascale systems include hundreds of thousands of nodes and millions of
cores\cite{dongarra:exascale-talk-2010}. To maximize scalability, an API must provide
demultiplexing\note{Patrick, what do you have in mind here?}, use shared resources as much
as possible, and support NUMA/multicore systems.

\subsection{Robustness}
A robust API should include connection-oriented semantics, error recovery, and support for
unreliable communications including multicast.

A new API should not hide the connection handling under the covers like MPI or Portals.
As with sockets, using connection-oriented semantics is familiar and well understood. We
do not mean that an implementation must maintain connection state. It is possible that
some implementations might handle connection state in a kernel driver or in the NIC or not
require any at all. The idea of requiring the connection-oriented semantics is that a
\emph{client} connects to a \emph{server} and the server chooses to accept or reject the
client's request. It is also an opportunity to allow the client to pass credentials to the
server if authentication and/or authorization is required and it is an opportunity for the
two sides to negotiate message sizes, the number of messages in flight, and other internal
information as appropriate.

As systems scale larger, error recovery needs to be simple and well-defined.\note{we need
more here}

Not all applications require reliable communications such as high-frequency financial
trading, video delivery, and distributed, system-health monitoring services that want the
most recent data and cannot wait for retransmission. Given that, an API should provide
unreliable connections in addition to reliable ones. Support should also be provided for
unreliable multicast if the hardware supports it.

\section{The CCI Interface}
In this section, we provide a brief overview of the CCI API to allow us to discuss how CCI
can meet the goals outlined above. Each function call is prefixed by \texttt{cci\_} which
we omit below for brevity.

\subsection{Initialization}
Before calling any function, the application must call \f{init}. The application
may call init multiple times with different parameters. The application can then call
\f{get\_devices} to obtain an array of available devices. The devices are parsed
from a config file and each device has a name, an array keyword/value strings, a maximum
send size in bytes, and PCI information if needed. Each device's maximum send size is
equivalent to the network MTU. When no more communication is needed, the application calls
\f{free\_devices}.

\subsection{Communication Endpoints}
All communication in CCI revolves around an endpoint. Each endpoint has some number of
device-sized buffers available for sending and receiving messages of small, unexpected
messages. The application calls \f{create\_endpoint} and \f{destroy\_endpoint},
respectively, to obtain or release an endpoint. The application may alter the number of
send and/or receive buffers using \f{get\_opt} and \f{set\_opt}.

\subsection{Event Handling}
CCI is inherently asynchronous and all communication functions only initiate
communication. When a communication completes, it generates an event. There are three
event types: SEND, RECV, and OTHER. The OTHER event returns connection success, rejection
or timeout events as well as endpoint and/or device failure events.

An application can poll for an event with \f{get\_event}, which returns an event structure
of which the contents vary depending on the event's type. When a process is finished with
an event, it uses \f{return\_event} to release it resources, if any, back to CCI.

In addition to returning an endpoint, \f{create\_endpoint} also returns an operating
system-specific handle that can be passed to \f{select} or other OS functions to allow
blocking until an event is available.

\subsection{Connections}
CCI defines a \emph{connection} struct which includes the maximum send size negotiated by
the two instances of CCI, a pointer to the owning endpoint, and the connection attributes.

As mentioned above, some applications may need reliable delivery while other may not.
Among applications needing reliable delivery, some may need in-order completion (e.g.
traditional SOCK\_STREAM semantics) and others may accept out-of-order completion as long
as communications are initiated in-order (e.g. MPI point-to-point).

In order to provide applications with as much or as little guarantees as possible, CCI
provides multiple types of connection attributes:

\begin{itemize}
\item Reliable with Ordered completion (RO)
\item Reliable with Unordered completion (RU)
\item Unreliable with Unordered completion (UU)
\item Unreliable with Unordered completion with multicast send (UU\_MC\_TX)
\item Unreliable with Unordered completion with multicast receive (UU\_MC\_RX)
\end{itemize}

If a process needs a mix of types, it is allowed to open multiple connections to the other
process.

\subsection{Connection Handling}
As we previously discussed, part of the robustness of the sockets API is its client/server
connection semantics. CCI mirrors those semantics. A process willing to accept connections
will first \f{bind} a device to a name service at a specified \emph{port} and a
\emph{backlog} parameter. The call returns a pointer to a \emph{service}. When a server no
longer wishes to receive connection requests, it can \f{unbind} from the service.

To initiate a connection, the client calls \f{connect} with parameters including an
endpoint, a string URI for the server, the port, optionally a pointer to a limited sized
payload and its length, the connection attribute, a pointer to an optional application
context, and a timeout.

The server then polls for connection requests using \f{get\_conn\_request} passing in the
service pointer. If one is ready, it returns a \emph{conn\_req} struct which contains an
array of compatible devices, the number of devices in the array, a pointer to the
application payload and its length if the client sent it, and the requested connection
attribute.

The server then calls either \f{accept} or \f{reject}. The \f{accept} call binds the
connection request to an endpoint previously created from one of the compatible devices
and returns a connection pointer. The client gets an OTHER event with the type
CONNECT\_SUCCESS. If the server calls \f{reject}, the client get an other event with the
type CONNECT\_REJECTED.  On the server, the connection request is stale after either call.
If the server does not reply within the timeout set in the client's \f{connect}, the
client gets an OTHER event with a type of CONNECT\_TIMEOUT. When a process no longer needs
a connection, it can call \f{disconnect}.

\subsection{Active Messages}
Once the connection is established, the two processes can start communications. CCI
provides two methods, active messages and remote memory access (RMA), which we discuss in
the next section.

CCI's version of active messages does not fully mirror Active Messages\cite{am} (AM). Like
the original AM, CCI's active messages have a maximum size that is device dependent.
Ideally, the size is equal to the link MTU (less wire headers). The driving idea to
limiting the message size to a single MTU is that future networks may have many paths
through the network due to fabrics with high-radix switches and/or NICs with multiple
ports connected to redundant switches for fault-tolerance.  Limiting the active message
size limited to a single MTU vastly simplifies the requirements for message completion ---
either it arrives or it does not.

Where CCI differs from the original Active Messages is handling of incoming messages.  In
Active Messages, the message contains a pointer to the handler that will process it.  The
difficulty with invoking handlers is there is no bound on how long the handler will run.
While running, the communication library cannot process any more messages and could lead
to dropping messages. Instead, CCI returns an event of type RECV.

The \f{send} parameters include the connection, header and data pointers and their
respective lengths, an application context pointer, and flags. Either or both of the
pointers may be NULL. The header is currently limited to a maximum length of 32 bytes. The
context pointer is returned in the SEND completion event and can be used to allow the
application to retrieve its internal state.

The optional flags parameter can accept the following:

\begin{description}
\item BLOCKING which means that the send should not return until the send
completes. The send completion status is passed in the function's return value.
\item NO\_COPY is a hint to CCI that the application does not need the buffer
back until the send completes and is free to use zero-copy methods if supported.
\item SILENT indicates that the process does not want a completion event for this
send.
\end{description}

On the receiver, a call to \f{get\_event} returns a RECV event which includes pointers to
the header and data, their lengths, and a pointer to the connection. The receiving process
can choose to simply inspect the data in-place, modify the data in-place and send it to
another process, or copy it out if it needs to keep the data long-term. When the process
no longer needs the buffer, it releases it back to CCI with \f{return\_event}. It should
be noted that if the application does not process RECV events and return them to CCI fast
enough, that CCI may still need to drop incoming messages.

CCI also provides a \f{sendv} function that takes an array of data pointers and an array
of lengths instead of the just the one data pointer and length in \f{send}. Lastly, CCI
does not require memory registration for sending or receiving active messages.

\subsection{RMA}
Clearly, messages limited to a single MTU will not meet the needs of all applications.
Applications such as file systems which need to move large, bulk messages need much more.
To accommodate them, CCI also provides remote memory access (RMA). RMA transfers are only
allowed on reliable connections.

Before using RMA, the process needs to explicitly register the memory. CCI provides
\f{rma\_register} which takes pointers to the endpoint, the connection, and the start of
the region to be registered as well as the length of the region. It returns a RMA handle.
When a process no longer needs to RMA in to or out of the region, it passes the handle to
\f{rma\_deregister}.

For a RMA transfer to take place, both processes must register their local memory and they
need to pass the handle of the target process to the initiator process using one or more
active messages. The full \f{rma} signature is:

\begin{verbatim}
int rma(cci_connection_t *connection,
        void *header_ptr,
        uint32_t header_len,
        uint64_t local_handle,
        uint64_t local_offset,
        uint64_t remote_handle,
        uint64_t remote_offset,
        uint64_t data_len,
        void *context,
        int flags);
\end{verbatim}

\section{Evaluation}
Text goes here

\section{Conclusion}
The conclusion goes here.


% use section* for acknowledgement
\section*{Acknowledgment}

The authors would like to thank...


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{30}

\bibitem{mpi-ft}
R. Batchu et al, ``MPI/FT TM : Architecture and taxonomies for fault-tolerant,
message-passing middleware for performance-portable parallel computing'', in proceedings
of \emph{1st IEEE International Symposium of Cluster Computing and the Grid}, 2001, pp.
26-33.

\bibitem{lnet}
P. Braam, P. Schwan, and R. Brightwell, ``Portals and Networking for the Lustre File
System'', in proceedings of \emph{IEEE International Conference on Cluster Computing},
2002.

\bibitem{portals}
R. Brightwell, R. Reisen, B. Lawry, and A. B. Maccabe, ``Portals 3.0: Protocol Building
Blocks for Low Overhead Communication'', in proceedings of \emph{2002 Workshop on
Communication Architecture for Clusters}, 2002.

\bibitem{cielo}
Cielo, Los Alamos National Lab, \url{http://www.lanl.gov/programs/asc/cielo_capability.shtml}.

\bibitem{dongarra:exascale-talk-2010}
J. Dongarra, ``Impact of Architecture and Technology for Extreme Scale on Software
and Algorithm Design'', in the proceedings of \emph{The Department of Energy Workshop on
Cross-cutting Technologies for Computing at the Exascale}, Feb. 2010.

\bibitem{ft-mpi}
G. Fagg and J. Dongarra, ``FT-MPI: Fault Tolerant MPI, supporting dynamic applications in
a dynamic world'', 2000.

\bibitem{gasnet}
GASNet, \url{http://gasnet.cs.berkeley.edu/}.

\bibitem{intel-mpi}
Intel MPI Library, Intel, Inc.,
\url{http://software.intel.com/en-us/articles/intel-mpi-library/}.

\bibitem{jaguar}
Jaguar, National Center for Computational Science, Oak Ridge National Lab,
\url{http://www.nccs.gov/computing-resources/jaguar/}.

\bibitem{mpi}
Message Passing Interface Forum. ``MPI: A message-passing interface standard'',
\emph{Technical Report}, 1994.

\bibitem{mpich2}
MPICH2 development team, MPICH2, \url{http://www.mcs.anl.gov/mpi/mpich2}.

\bibitem{mvapich}
MVAPICH, Department of Computer Science and Engineering, Ohio State University,
\url{http://mvapich.cse.ohio-state.edu/}.

\bibitem{mx}
MyrinetExpress (MX), Myricom, Inc., \url{http://www.myri.com/}.

\bibitem{ofa-verbs}
Open Fabrics Alliance, \url{http://www.openfabrics.org}.

\bibitem{ompi}
Open MPI: Open source high performance computing, \url{http://www.open-mpi.org}.

\bibitem{platform-mpi}
Platform MPI, Platform Computing, Inc.,
\url{http://www.platform.com/cluster-computing/platform-mpi}.

\bibitem{psm}
PathScale Messages (PSM), Qlogic, Inc.,
\url{http://www.qlogic.com/SiteCollectionDocuments/Products/Products_RightNAV_pdfs/infiniband%20hcas/IB6054601-00.pdf}.

\bibitem{srq}
S. Sur et al, ``Shared Receive Queue Based Scalable MPI Design for InfiniBand Clusters'',
in the proceedings of \emph{International Parallel and Distributed Processing Symposium
(IPDPS)}, 2006.

\bibitem{top500}
Top 500 Interconnect Family share for 11/2010,
\url{http://www.top500.org/stats/list/36/connfam}.

\bibitem{via}
T. von Eicken and W. Vogels, ``Evolution of the Virtual Interface Architecture'', in
\emph{IEEE Computer}, Vol. 31, Nov. 1998, pp. 61-68. 




\end{thebibliography}

% that's all folks
\end{document}
