\section{State of the Art}
\label{sec:state}
Over the years, many communication interfaces have come and gone. The few that have
remained and seen wide-spread adoption are BSD Sockets~\cite{Sechrest:CSD-84-191}, the Message Passing
Interface (MPI)~\cite{mpi_forum93:_mpi}, and some vendor-specific application programming interfaces
(API).

\subsection{Sockets} The Sockets interface is the most widely used by far. All major
operating systems provide support for Sockets; the Internet and all the services it
provides would not exist without it. The popularity of BSD Sockets can be attributed to:

\begin{itemize}
\item Simple API
\item Robustness
\item Implicit buffering
\end{itemize}

The API provides stream and datagram based modes, connection-oriented and connection-less
modes, and client/server semantics for connection-oriented modes. Based on the transport,
the API can supports multiple delivery modes (unicast, multicast, and/or broadcast). The
API does not provide for collective communication nor does it provide one-sided
operations.

Sockets implementations are mature and well understood. It does not assume or require
special hardware features (nor can it exploit them if they exist).

Both sends and receives are buffered allowing operations to complete immediately (if buffer
space is available for sends or data exists in the buffer for receives). Applications may
also choose to not block if send buffers are full or receive buffers are empty if need be.
The downside of buffering is more work is required by the CPU which can result in lower
throughput over the network.

Sockets, when used with SOCK\_STREAM, inherits the well-known TCP
performance constraints~\cite{Foong03tcpperformance} related to the
scaling window and the bandwidth delay product. 

\subsection{MPI}
\label{sec:mpi} 

In the High Performance Computing (HPC) arena, MPI is the dominant
interface for inter-process communication. Designed for maximum scalability, MPI has a
richer, but much more complicated API than Sockets.

MPI provides point-to-point (i.e. send-recv or two-sided semantics), collective, and
one-sided operations. For point-to-point communication, MPI provides a variety of modes
including blocking and non-blocking, synchronous and asynchronous, as well as \emph{ready}
mode (only send if a matching receive has been posted).

Multiple implementations exist~\cite{ompi_04_pvmmpi_overview, Gropp:1996:HPI, Liu:2003:RDMA}
which support multiple operating systems and/or interconnects.  Rather than connections,
the API uses the notion of communication groups (i.e.  communicators) which include all
processes ({\sf MPI\_COMM\_WORLD}) and may be split to include subsets of processes. MPI does
provide a notion of dynamic process management which includes {\sf MPI\_COMM\_ACCEPT} and
{\sf MPI\_COMM\_CONNECT}, but this is less mature and much less used.

The MPI standard does not define an underlying network protocol and each MPI
implementation has written its own network abstraction layer (NAL). These NALs typically
support BSD Sockets as well as one or more vendor specific APIs.

MPI is a less mature technology -- compared to Sockets -- likely due to its
relative complexity (over 300 functions), niche market penetration
(HPC), and relative youth. While MPI semantics are fairly well defined
by the MPI standard, implementations vary in their compliance to the
MPI specification and may exhibit differing behavior when the standard is
silent on a specific semantic. Applications that rely upon the
semantic embodied in a particular implementation may not be portable
across other MPI implementations. MPI provides a rigid fault model in
which a process fault within a communication group imposes failure to
all processes within that communication group
({\sf MPI\_ERRORS\_ABORT}). Although work has been done on fault-tolerant
MPI~\cite{fagg04:_fault_toler_commun_librar_applic_high_perof, mpi-ft},
that work has yet to be adopted by the broader HPC community. 

The MPI standard remains silent on a number of areas that are often
performance critical. For basic send/recv operations MPI
implementations often adopt two common strategies that attempt to
balance buffering and communication overhead. ``Eager'' mode will send
data immediately to the receiver regardless of the receiver having
posted a matching receive. The data is buffered on the receiver if the
matching receive is not posted upon receipt of the
data. ``Rendezvous'' mode defers sending the data until a posted
receive is matched on the receiver. Performance of an MPI application
can therefore be largely implementation-dependent and may vary even
within a single implementation depending on the MPI configuration
settings used for a particular invocation. Perhaps more importantly,
this ambiguity can result in receiver buffer overruns or out-of-memory
(OOM) faults on the receiver.

MPI has added support for one-sided operations as currently defined
in the MPI-2 standard~\cite{geist96:_mpi2_lyon}. The MPI-2 one-sided
interface has received limited adoption due to API complexity and
overly constrained semantics~\cite{bonachea-upc-mpi2}. Current efforts
in the MPI-3 standardization process are aimed at addressing these
issues. 


\subsection{Vendor APIs} There are numerous vendor- and
organization-specific APIs available today including OpenFabrics
Verbs, Cray/Sandia's Portals~\cite{portals}, Qlogic's
PSM, Myricom's MX, LBL's GASNet~\cite{gasnet}, 
DAPL, IBM's LAPI~\cite{lapi_a_1998}, IBM's 
DCMF~\cite{Kumar:2008:DCM:1375527.1375544} and so on. 
Overall, they provide a lot of choice but none has appeared as 
a widely used communication interface. Since most are targeted to 
specific network technologies, the APIs tend to reflect the design 
of the underlying hardware. In the rest of this section, we will 
present each of these interfaces.

Based on the earlier VIA specification~\cite{via}, the InfiniBand specification does not specify
an API; it only specifies which \emph{Verbs} must be supported. After many vendors created
separate Verbs APIs, they eventually coalesced into the OpenFabrics Association's (OFA)
Verbs. Verbs has support for two-sided and one-sided operations, always asynchronous. 
In addition, Verbs has support for reliable and unreliable modes, connection-oriented
and connection-less. Buffer management is left to the application, all receives must be 
posted before sending. Also, all data movement operations require registering the memory 
in advance. Verbs use the concept of Queue Pair (QP) represent a logical connection between 
two processes.

The Portals API provides one-sided semantics (i.e.  Put/Get) but uses match tags to steer
messages to the correct buffers. The API is connection-less and leaves it up to the NAL to
maintain any necessary connection state internally. Portals is mostly used on the large
Cray systems such as ORNL's Jaguar~\cite{jaguar_cug_2010} and LANL's Cielo.  The
Lustre distributed file system NAL, LNET, was originally based on Portals~\cite{lnet}.

Both designed for efficiently implementing MPI, Myricom's MyrinetExpress (MX) and 
Qlogic's PathScale Messages (PSM) have many similarities. Both provide a two-sided matching 
interface which uses buffering for smaller messages and zero-copy for larger messages.  
Both are connection-less in that the target does not have to accept.  Both provide reliable,
in-order matching, but out-of-order completion.

LAPI~\cite{lapi_a_1998} and DCMF~\cite{Kumar:2008:DCM:1375527.1375544}
are both proprietary software stack developed by IBM, using as target
the RS-series and lately the BlueGene P/Q IBM machines. While some support
from the scientific community outside IBM existed, it failed to
broaden and remains limited. DCMF do support an
interface for one-sided and two-sided message semantic, with
contiguous or discontiguous memory layout, providing transparent support 
for link aggregation. The DCMF communication layer supports 
multiple programming paradigms such as Aggregate Remote Memory Copy Interface 
(ARMCI) and Global Arrays (GA), in addition to MPI. It also provides 
a collective API, allowing processes to execute asynchronous collective
communications in an optimized way.

Among all of these interfaces, OFA's Verbs has the broadest adoption in HPC, 
representing 43\% of the machines in the Top 500~\cite{top500}. Despite 
a limited success in the enterprise world, Verbs aspire to widen its audience 
beyond HPC, especially over Ethernet through RoCEE~\cite{RoCEE}. As such, we 
will refer to Verbs alongside Sockets and MPI in the rest of the document.
