
\section{Status and Evaluation}
We have two partial, proof-of-concept implementations, one over UDP sockets and
one over Portals 3.3.

The socket prototype driver opens one SOCK\_DGRAM (UDP) socket per CCI endpoint
and has to provide reliability (acknowledgement and retransmission on loss).
The socket driver also has to multiplex connections over the single socket.
Enough functions are complete including RMA write to allow simple pingpong
tests to exercise the CCI API.

The Portals implementation also implements enough functions to run a pingpong
test with active messages. Since Portals assumes a reliable interconnect, the
only difference between a CCI UU connection and a RO/RU connection is that we
complete a UU send when we receive the Portals' SEND\_END event which indicates
that the sender's buffer is no longer needed (i.e. the data is on the wire).
For a reliable connection, we report a completion when we get the Portals' ACK
of receipt at the peer. We wrote a native Portals pingpong and compared it to
the CCI pingpong performance. We ran tests on a Cray XT6 and locked the
processes to the same cores for both tests. The CCI over Portals
\begin{math}\frac{1}{2}\end{math}RTT was less than 200 ns more than the native
Portals' performance up to 256 bytes. For an eight byte message, for example,
CCI's latency is 5.91 \us versus 5.74 \us for native Portals. Figures
\ref{fig:latency} and \ref{fig:bw} show latency and bandwidth for active messages
up to 8 KB on a Cray XT6.

Given the authors work with MX, GM, sockets, and Portals, we expect that a BTL
implementation using CCI would be smaller than MX and hopefully smaller than
Elan.

CCI uses connection-oriented semantics with minimal per-connection resources.
Firgure \ref{memory} shows the connection state (not including buffers) for
Verb's X-SRQ (single shared receive queue per node and per connection send QP),
a SOCK\_STREAM socket, and CCI's connection state (assuming 256 bytes due to
alignment constraints).

\subsection{Portability}

\subsection{Performance}

\begin{figure}[htbp]
\centering
\includegraphics[width=3.5in]{pingpong-latency.eps}
\caption{Pingpong Latency when using Portals on SeaStar}
\label{fig:latency}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=3.5in]{pingpong-bw.eps}
\caption{Pingpong Bandwidth when using Portals on SeaStar}
\label{fig:bw}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=3.5in]{memory_log.eps}
\caption{Memory footprint for connections per process}
\label{fig:memory}
\end{figure}

\subsection{Scalability}

For the Portals driver, each connection requires 104 bytes on 64-bit machines
(20 bytes for the public CCI connection struct, 20 bytes for the private CCI
struct, and 64 bytes for the Portals driver connection struct).  The sock driver
needs 140 bytes for each connection.

\subsection{Robustness}

